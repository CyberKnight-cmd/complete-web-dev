# Sitemaps & robots.txt (intro)

## Goal

Master **Sitemaps & robots.txt (intro)** within **Phase 2 — HTML + CSS**. Depth requirements include: explaining concepts from first principles, applying them in code without reference materials, and diagnosing/debugging common sitemap and robots.txt implementation failures.

## Learning Objectives

* Explain the purpose and mechanics of sitemaps and robots.txt from first principles
* Apply sitemap XML structure and robots.txt directives correctly without reference
* Implement proper crawling guidance and site discovery mechanisms
* Debug common sitemap and robots.txt issues (malformed XML, incorrect directives, crawling blocks)

## Coverage

* Clear definitions of site discovery: XML sitemaps, robots.txt protocol, crawling directives
* Sitemap structure: XML format, URL priorities, change frequencies, last modification dates
* Robots.txt syntax: user-agent directives, disallow rules, sitemap declarations
* Mental models for how search engines discover and crawl website content
* Minimal demo code samples illustrating proper sitemap and robots.txt implementation
* Typical pitfalls and troubleshooting patterns (malformed XML, overly restrictive robots.txt, missing sitemaps, incorrect paths)

## Deliverables

* 1–2 page notes capturing core sitemap and robots.txt concepts and implementation strategies
* Runnable, minimal examples demonstrating proper sitemap XML and robots.txt configuration in isolation
* Screenshots/GIFs of sitemap validation, robots.txt testing, and crawling verification

## Acceptance Criteria

* README with setup and run instructions (including how to validate sitemaps and test robots.txt)
* Include tests where applicable (sitemap validators, robots.txt testers, crawling simulation tools)
* Demonstrate understanding through working examples and accompanying documentation